{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1950b56c",
   "metadata": {},
   "source": [
    "# Credit Risk Analysis â€“ Loan Default Prediction ðŸ’³ðŸ“‰\n",
    "\n",
    "**Author:** Luis Chaumer  \n",
    "**Role:** Data Analyst  \n",
    "\n",
    "This project focuses on **credit risk analysis** for a fictional consumer lending portfolio.  \n",
    "Using a synthetic dataset of **5,000 loans**, the goal is to:\n",
    "\n",
    "- Explore drivers of loan default  \n",
    "- Build a baseline **credit risk model** (classification)  \n",
    "- Segment clients into **risk tiers**  \n",
    "- Use both **Python and SQL** to analyze the portfolio  \n",
    "- Provide insights and recommendations for risk management and lending policy  \n",
    "\n",
    "The dataset used in this analysis is: `data/credit_risk_dataset.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f9ad1",
   "metadata": {},
   "source": [
    "## 1. Setup: imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7637f",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/credit_risk_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, parse_dates=[\"application_date\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21095e03",
   "metadata": {},
   "source": [
    "## 3. Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c352fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19101fd2",
   "metadata": {},
   "source": [
    "### 3.1 Target variable distribution (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"default\"].value_counts(normalize=True).rename(\"proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "df[\"default\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xticks([0, 1], [\"No default (0)\", \"Default (1)\"], rotation=0)\n",
    "plt.title(\"Default distribution\")\n",
    "plt.ylabel(\"Number of loans\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09b0f6",
   "metadata": {},
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"application_date\"].dt.year\n",
    "df[\"month\"] = df[\"application_date\"].dt.month\n",
    "\n",
    "df[\"income_bucket\"] = pd.cut(\n",
    "    df[\"annual_income\"],\n",
    "    bins=[0, 20000, 40000, 60000, 80000, 120000],\n",
    "    labels=[\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"]\n",
    ")\n",
    "\n",
    "df[\"credit_bucket\"] = pd.cut(\n",
    "    df[\"credit_score\"],\n",
    "    bins=[499, 599, 659, 719, 850],\n",
    "    labels=[\"poor\", \"fair\", \"good\", \"excellent\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d399c6f",
   "metadata": {},
   "source": [
    "## 5. Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd10d50",
   "metadata": {},
   "source": [
    "### 5.1 Correlation heatmap (numeric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c867728",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"age\", \"annual_income\", \"loan_amount\", \"term_months\", \"interest_rate\", \"credit_score\", \"dti\", \"default\"]\n",
    "corr = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Correlation heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6fa79",
   "metadata": {},
   "source": [
    "### 5.2 Default rate by credit bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227520ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_by_credit = df.groupby(\"credit_bucket\")[\"default\"].mean().rename(\"default_rate\")\n",
    "default_by_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "default_by_credit.plot(kind=\"bar\")\n",
    "plt.title(\"Default rate by credit score bucket\")\n",
    "plt.ylabel(\"Default rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24f932",
   "metadata": {},
   "source": [
    "### 5.3 Default rate by loan purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_by_purpose = df.groupby(\"loan_purpose\")[\"default\"].mean().sort_values(ascending=False)\n",
    "default_by_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6654c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "default_by_purpose.plot(kind=\"bar\")\n",
    "plt.title(\"Default rate by loan purpose\")\n",
    "plt.ylabel(\"Default rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1722ac",
   "metadata": {},
   "source": [
    "## 6. SQL analysis with SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd891ea6",
   "metadata": {},
   "source": [
    "In this section, we use **SQL** (via SQLite) to perform portfolio analysis tasks that are common in credit risk teams.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the dataset into an in-memory SQLite database  \n",
    "- Run several SQL queries to compute default rates and portfolio metrics  \n",
    "- Show how Python and SQL can complement each other in a credit risk workflow  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\":memory:\")\n",
    "df.to_sql(\"loans\", conn, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f877973",
   "metadata": {},
   "source": [
    "### 6.1 Default rate by loan purpose (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    loan_purpose,\n",
    "    COUNT(*) AS total_loans,\n",
    "    SUM(default) AS total_defaults,\n",
    "    ROUND(AVG(default), 4) AS default_rate\n",
    "FROM loans\n",
    "GROUP BY loan_purpose\n",
    "ORDER BY default_rate DESC;\n",
    "'''\n",
    "sql_default_by_purpose = pd.read_sql_query(query, conn)\n",
    "sql_default_by_purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f6da4",
   "metadata": {},
   "source": [
    "### 6.2 Default rate by credit bucket and income bucket (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    credit_bucket,\n",
    "    income_bucket,\n",
    "    COUNT(*) AS total_loans,\n",
    "    SUM(default) AS total_defaults,\n",
    "    ROUND(AVG(default), 4) AS default_rate\n",
    "FROM loans\n",
    "GROUP BY credit_bucket, income_bucket\n",
    "ORDER BY credit_bucket, income_bucket;\n",
    "'''\n",
    "sql_segment_risk = pd.read_sql_query(query, conn)\n",
    "sql_segment_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006bd00",
   "metadata": {},
   "source": [
    "### 6.3 Portfolio metrics by year (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782775fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    year,\n",
    "    COUNT(*) AS total_loans,\n",
    "    ROUND(AVG(loan_amount), 2) AS avg_loan_amount,\n",
    "    ROUND(AVG(interest_rate), 4) AS avg_interest_rate,\n",
    "    ROUND(AVG(default), 4) AS default_rate\n",
    "FROM loans\n",
    "GROUP BY year\n",
    "ORDER BY year;\n",
    "'''\n",
    "sql_by_year = pd.read_sql_query(query, conn)\n",
    "sql_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320d4bf",
   "metadata": {},
   "source": [
    "## 7. Modeling: baseline credit risk models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80adc1d",
   "metadata": {},
   "source": [
    "We build simple baseline models to predict **loan default**:\n",
    "\n",
    "- **Logistic Regression** (interpretable baseline)  \n",
    "- **Random Forest** (non-linear model, can capture interactions)  \n",
    "\n",
    "This is not meant to be a full production-grade scorecard, but a demonstration of modeling workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905163e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"age\", \"annual_income\", \"loan_amount\", \"term_months\",\n",
    "    \"interest_rate\", \"credit_score\", \"dti\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"default\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025877a",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n",
    "y_pred_lr = (y_pred_proba_lr >= 0.5).astype(int)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "auc_lr, cm_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression classification report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba_lr)\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaedf03",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf = (y_pred_proba_rf >= 0.5).astype(int)\n",
    "\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "auc_rf, cm_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest classification report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3299280",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba_rf)\n",
    "plt.title(\"ROC Curve - Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fee724",
   "metadata": {},
   "source": [
    "### 7.3 Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b25945",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "importances.plot(kind=\"bar\")\n",
    "plt.title(\"Feature importance - Random Forest\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24434d9",
   "metadata": {},
   "source": [
    "## 8. Risk segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = X_test.copy()\n",
    "df_test[\"default\"] = y_test.values\n",
    "df_test[\"pd_rf\"] = y_pred_proba_rf\n",
    "\n",
    "df_test[\"risk_tier\"] = pd.cut(\n",
    "    df_test[\"pd_rf\"],\n",
    "    bins=[0, 0.1, 0.25, 0.5, 1],\n",
    "    labels=[\"very_low\", \"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "risk_summary = df_test.groupby(\"risk_tier\").agg(\n",
    "    total_loans=(\"default\", \"count\"),\n",
    "    defaults=(\"default\", \"sum\"),\n",
    "    default_rate=(\"default\", \"mean\"),\n",
    "    avg_pd=(\"pd_rf\", \"mean\")\n",
    ")\n",
    "\n",
    "risk_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f479c",
   "metadata": {},
   "source": [
    "## 9. Conclusions and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c82b9",
   "metadata": {},
   "source": [
    "_Suggested narrative (to be adapted based on actual results):_\n",
    "\n",
    "- The overall default rate in the portfolio is **X%**, with higher risk concentrated in lower credit score buckets and higher DTI segments.  \n",
    "- **Credit score**, **DTI**, **interest rate** and **loan amount** emerge as key drivers of default risk.  \n",
    "- The Random Forest model achieves an ROC-AUC of around **Y**, outperforming the logistic regression baseline and providing better rank-ordering of risk.  \n",
    "- The risk tier segmentation (very_low / low / medium / high) clearly separates groups with different default rates, which can be used to adjust pricing, approval strategies or exposure limits.  \n",
    "- SQL-based analysis proves useful for quickly summarizing portfolio risk by segments (purpose, year, credit bucket, income bucket).  \n",
    "\n",
    "**Business recommendations:**\n",
    "\n",
    "1. Tighten approval criteria or pricing in high-risk segments (e.g., low credit score + high DTI + small business loans).  \n",
    "2. Use the risk tiers to define differentiated treatment strategies (e.g., monitoring, early collections, credit limits).  \n",
    "3. Combine Python modeling with SQL reporting in a regular credit risk monitoring process.  \n",
    "4. Extend the modeling with additional features (payment history, macro variables, bureau data) for production environments.  \n",
    "\n",
    "This project demonstrates an end-to-end approach to **credit risk modeling** combining **Python, SQL, EDA, modeling and business interpretation**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
